# Entrenar IA ‚Äî Fine-tuning de modelo Qwen 2.5 con LoRA

![Fine-tuning LoRA sobre Qwen 2.5 ‚Äî entrenamiento personalizado de modelo de IA local](https://img.shields.io/badge/Python-LoRA_Fine--tuning-3776AB?style=for-the-badge&logo=python&logoColor=white)

> üîó **GitHub Pages:** [https://mutenros.github.io/Programacion-005-Entrenar-IA/](https://mutenros.github.io/Programacion-005-Entrenar-IA/)

## Introducci√≥n

Este proyecto es un recorrido completo por el proceso de personalizaci√≥n de un modelo de inteligencia artificial: desde la preparaci√≥n de los datos de entrenamiento (JSONL), pasando por el fine-tuning con LoRA/QLoRA en Python, la inferencia del modelo resultante, hasta la construcci√≥n de una interfaz web tipo chat que consume el modelo. El proyecto cubre 10 ejercicios progresivos m√°s archivos auxiliares (servidor Flask, m√≥dulo de inferencia) y un frontend HTML/CSS/JS. Demuestra que es posible entrenar un modelo de IA con datos propios en un equipo local usando t√©cnicas eficientes como LoRA, sin necesidad de GPUs profesionales ni servicios en la nube.

---

## Desarrollo de las partes

### 1. Chat PHP con Ollama ‚Äî Base del proyecto (001, 002)

Los dos primeros ejercicios son aplicaciones PHP que conectan con Ollama (igual que en el proyecto anterior). La diferencia clave del ejercicio 002 es la instrucci√≥n adicional "no infieras": se le dice al modelo que si no conoce la respuesta, lo indique expl√≠citamente.

```php
// Archivo: 101-Ejercicios/002-no inferencia.php, l√≠neas 9-11
$prompt = $userPrompt . "
  - responde en un solo p√°rrafo, sin c√≥digo, en prosa.
  - respuestas solo en espa√±ol
  - no infieras, si no conoces la respuesta, ind√≠calo";
```

Esta instrucci√≥n es fundamental: demuestra por qu√© necesitamos entrenar el modelo con datos propios ‚Äî un modelo gen√©rico no conoce informaci√≥n espec√≠fica sobre una persona o instituci√≥n.

---

### 2. Plantilla de entrenamiento JSONL (003)

El formato de datos de entrenamiento es JSONL (JSON Lines): cada l√≠nea es un par pregunta-respuesta independiente. La plantilla muestra la estructura m√≠nima que espera el script de entrenamiento.

```jsonl
// Archivo: 101-Ejercicios/003-plantilla de entrenamiento.jsonl
{"question": "X", "answer": "Y"}
```

---

### 3. Dataset personalizado ‚Äî 20 pares QA (004)

Se rellena la plantilla con 20 pares pregunta-respuesta sobre Jose Vicente Carratal√° Sanchis: su profesi√≥n, tecnolog√≠as, proyectos, estilo pedag√≥gico, etc. Este dataset es lo que el modelo "aprender√°" en el fine-tuning.

```jsonl
// Archivo: 101-Ejercicios/004-plantilla rellenada con mis datos.jsonl, l√≠neas 1-3
{"question":"¬øQui√©n es Jose Vicente Carratal√° Sanchis?","answer":"Jose Vicente Carratal√° Sanchis es un desarrollador de software, formador y creador de contenidos t√©cnicos especializado en programaci√≥n, sistemas e inteligencia artificial."}
{"question":"¬øA qu√© se dedica Jose Vicente Carratal√° Sanchis profesionalmente?","answer":"Jose Vicente Carratal√° Sanchis se dedica al desarrollo de software, a la formaci√≥n t√©cnica en programaci√≥n y a la creaci√≥n de proyectos tecnol√≥gicos educativos."}
{"question":"¬øQu√© tipo de software desarrolla Jose Vicente Carratal√° Sanchis?","answer":"Jose Vicente Carratal√° Sanchis desarrolla aplicaciones web, plataformas educativas, herramientas de gesti√≥n empresarial y sistemas basados en inteligencia artificial."}
```

El dataset completo tiene 20 pares cubriendo: identidad, profesi√≥n, lenguajes, experiencia web, libros, Jocarsa, IA, modelos, administraci√≥n de sistemas, Moodle, gr√°ficos, preferencias de estilo, y el objetivo del entrenamiento.

---

### 4. Preparaci√≥n del entorno Python (005)

Script shell que documenta los comandos necesarios para crear un entorno virtual Python e instalar las dependencias: `torch`, `datasets`, `peft` y `transformers`.

```bash
# Archivo: 101-Ejercicios/005-preparamos entorno.sh, l√≠neas 3-13
python3 -m venv venv
source venv/bin/activate
pip install torch
pip install datasets
pip install peft
pip install transformers
```

---

### 5. Entrenamiento LoRA ‚Äî Script principal (006)

El archivo central del proyecto (276 l√≠neas). Carga el dataset JSONL, descarga el modelo base Qwen2.5-0.5B-Instruct, aplica adaptadores LoRA y entrena durante 80 √©pocas. Incluye la generaci√≥n autom√°tica de un informe Markdown con m√©tricas.

```python
# Archivo: 101-Ejercicios/006-entrenar el modelo - afinar el modelo.py, l√≠neas 18-42
DATA_PATH = "outputs/*.jsonl"
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
OUTPUT_DIR = "./qwen25-05b-jvc"

MAX_LENGTH = 512
NUM_EPOCHS = 80
LR = 2e-4
BATCH_SIZE = 1
GRAD_ACCUM = 4

LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
```

La configuraci√≥n LoRA aplica adaptadores a 7 capas del transformer (`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`) con rango 16 y alpha 32. El entrenamiento usa *answer-only loss*: solo se calcula la p√©rdida sobre los tokens de la respuesta, no del prompt.

```python
# Archivo: 101-Ejercicios/006-entrenar el modelo - afinar el modelo.py, l√≠neas 143-149
lora = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
)
```

La mejora a√±adida valida que el dataset contenga los campos `question` y `answer` antes de proceder:

```python
# Archivo: 101-Ejercicios/006-entrenar el modelo - afinar el modelo.py, l√≠neas 121-126
required_fields = {"question", "answer"}
missing = required_fields - set(raw.column_names)
if missing:
    print(f"‚ùå Error: faltan campos en el dataset: {missing}")
    return
```

---

### 6. Exportaci√≥n del modelo fusionado (007)

Despu√©s del entrenamiento, los adaptadores LoRA se fusionan con el modelo base usando `merge_and_unload()`, y se guarda el resultado como modelo completo independiente.

```python
# Archivo: 101-Ejercicios/007-exportar fusionado.py, l√≠neas 37-43
print("Fusionando LoRA en el modelo base (merge_and_unload)...")
merged = model.merge_and_unload()
merged.eval()

print("Guardando modelo fusionado en:", OUT_PATH)
merged.save_pretrained(OUT_PATH, safe_serialization=True)
```

El modelo fusionado (`.safetensors`) se puede usar sin necesidad de cargar el adaptador LoRA por separado, simplificando la inferencia.

---

### 7. Inferencia con el modelo entrenado (008, 009)

El ejercicio 008 es la inferencia b√°sica: carga el modelo fusionado, aplica el chat template de Qwen y genera con `temperature=0.6` y `do_sample=True`.

El ejercicio 009 mejora significativamente la calidad: usa generaci√≥n determinista (`do_sample=False`), limita a 64 tokens, a√±ade `repetition_penalty=1.05` y aplica post-procesado con `clean_answer()` que recorta respuestas verbosas y detecta el fallback.

```python
# Archivo: 101-Ejercicios/009-inferencia de mas calidad.py, l√≠neas 74-80
def clean_answer(text: str) -> str:
    if not text:
        return FALLBACK_EXACT
    t = text.strip()
    if FALLBACK_EXACT in t:
        return FALLBACK_EXACT
    first_line = t.splitlines()[0].strip()
    return first_line if first_line else FALLBACK_EXACT
```

---

### 8. Diagn√≥stico del entrenamiento (010)

Script avanzado que eval√∫a si el fine-tuning ha funcionado. Compara la *negative log-likelihood* de dos continuaciones alternativas: la respuesta correcta del dataset vs. una respuesta inventada.

```python
# Archivo: 101-Ejercicios/010-diagnostico.py, l√≠neas 98-114
nll_ok, len_ok, mean_ok = neg_loglik_of_continuation(model, tokenizer, score_prompt, ANSWER_OK)
nll_bad, len_bad, mean_bad = neg_loglik_of_continuation(model, tokenizer, score_prompt, ANSWER_BAD)

if nll_ok < nll_bad:
    print("RESULT: Model prefers the FINE-TUNED (OK) answer.")
else:
    print("RESULT: Model prefers the WRONG (BAD) answer -> fine-tune not applied.")
```

Si el modelo asigna mayor probabilidad a la respuesta correcta, el entrenamiento ha funcionado.

---

### 9. Servidor Flask ‚Äî API REST (server.py)

Un servidor Flask minimalista que expone la interfaz web y un endpoint `/api/chat` que recibe mensajes JSON y devuelve la respuesta del modelo.

```python
# Archivo: 101-Ejercicios/server.py, l√≠neas 14-23
@app.post("/api/chat")
def api_chat():
    data = request.get_json(force=True, silent=True) or {}
    message = (data.get("message") or "").strip()
    if not message:
        return jsonify({"ok": False, "error": "Empty message"}), 400
    try:
        answer = infer(message)
        return jsonify({"ok": True, "answer": answer})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500
```

---

### 10. Frontend web ‚Äî HTML, CSS y JavaScript

La interfaz web consta de tres archivos: `index.html` (estructura), `style.css` (257 l√≠neas con dise√±o tipo ChatGPT) y `app.js` (101 l√≠neas con l√≥gica de chat as√≠ncrono).

```javascript
// Archivo: 101-Ejercicios/web/app.js, l√≠neas 52-76
async function send() {
  const msg = (input.value || "").trim();
  if (!msg) return;
  addMessage("user", msg);
  messageCount++;
  setTyping(true);
  sendBtn.disabled = true;
  try {
    const res = await fetch("/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ message: msg })
    });
    const data = await res.json();
    setTyping(false);
    if (!data.ok) { addMessage("assistant", `Error: ${data.error}`); return; }
    addMessage("assistant", data.answer || "");
  } catch (e) {
    setTyping(false);
    addMessage("assistant", `Error: ${String(e)}`);
  } finally { sendBtn.disabled = false; input.focus(); }
}
```

El CSS incluye modo oscuro autom√°tico (`prefers-color-scheme: dark`), animaci√≥n de typing con parpadeo, transiciones en botones y dise√±o responsive.

---

### 11. M√≥dulo reutilizable de inferencia (infer.py)

M√≥dulo Python que encapsula la l√≥gica de inferencia con lazy-loading del modelo (singleton), prompt engineering estricto y limpieza de respuesta. Es importado tanto por `server.py` como ejecutable por s√≠ mismo.

```python
# Archivo: 101-Ejercicios/infer.py, l√≠neas 60-80
def load_model():
    global _TOKENIZER, _MODEL
    if _TOKENIZER is not None and _MODEL is not None:
        return _TOKENIZER, _MODEL
    _TOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
    _MODEL = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)
    _MODEL.eval()
    return _TOKENIZER, _MODEL
```

El patr√≥n singleton evita recargar el modelo en cada petici√≥n, ahorrando memoria y tiempo.

---

## Presentaci√≥n del proyecto

Este proyecto demuestra el flujo completo de personalizaci√≥n de un modelo de inteligencia artificial, de principio a fin. Partimos de un problema real: un modelo de IA gen√©rico no conoce informaci√≥n espec√≠fica sobre una persona o instituci√≥n. Cuando le preguntas "¬øQui√©n es Jose Vicente Carratal√°?", inventa o no sabe.

La soluci√≥n es entrenar el modelo con datos propios. Primero creamos un dataset JSONL con 20 pares pregunta-respuesta cuidadosamente redactados. Luego usamos LoRA (Low-Rank Adaptation), una t√©cnica de fine-tuning eficiente que solo entrena unos pocos millones de par√°metros en lugar de los miles de millones del modelo completo. Esto permite entrenar en un ordenador normal, incluso sin GPU dedicada.

El script de entrenamiento (006) carga el modelo base Qwen 2.5 de 0.5B par√°metros, le aplica adaptadores LoRA en 7 capas del transformer, y entrena durante 80 √©pocas optimizando solo la p√©rdida sobre los tokens de respuesta. Despu√©s, fusionamos los adaptadores con el modelo base (007) para obtener un modelo independiente.

Para verificar que el entrenamiento ha funcionado, el script de diagn√≥stico (010) compara la probabilidad que el modelo asigna a la respuesta correcta vs. una incorrecta. Si prefiere la correcta, el fine-tuning ha tenido √©xito.

Finalmente, exponemos el modelo a trav√©s de un servidor Flask con una API REST y una interfaz web tipo chat con dise√±o profesional, modo oscuro autom√°tico y comunicaci√≥n as√≠ncrona con fetch.

---

## Conclusi√≥n

Este proyecto muestra que el entrenamiento personalizado de modelos de IA es accesible para un estudiante de programaci√≥n. Con herramientas de c√≥digo abierto (Hugging Face Transformers, PEFT, PyTorch), un dataset modesto de 20 ejemplos y la t√©cnica LoRA, es posible ense√±arle a un modelo informaci√≥n nueva que no conoc√≠a. La cadena completa ‚Äî datos JSONL ‚Üí entrenamiento LoRA ‚Üí fusi√≥n ‚Üí inferencia ‚Üí servidor web ‚Üí interfaz chat ‚Äî demuestra la integraci√≥n de m√∫ltiples tecnolog√≠as (Python, Flask, HTML, CSS, JavaScript) en un proyecto cohesivo. El diagn√≥stico con negative log-likelihood a√±ade rigor al verificar cuantitativamente que el modelo ha aprendido, y la interfaz web permite cualquier persona interactuar con el resultado final.
